{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4b7af7",
   "metadata": {},
   "source": [
    "1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509d1d0",
   "metadata": {},
   "source": [
    "1.1 Domain-specific Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002196c",
   "metadata": {},
   "source": [
    "1.2 Description of the selected dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160dcdd",
   "metadata": {},
   "source": [
    "1.3 Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda711f",
   "metadata": {},
   "source": [
    "1.4 Evaluation Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9318d0",
   "metadata": {},
   "source": [
    "2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48515c2d",
   "metadata": {},
   "source": [
    "2.1 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d7acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mmenna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import hstack\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import ngrams\n",
    "from functools import reduce\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec98e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text, n = 1):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Convert text to lower case and remove all punctuation\n",
    "    2. Optionally apply stemming\n",
    "    3. Apply Ngram Tokenisation\n",
    "    4. Returns the tokenised text as a list \n",
    "    \"\"\"\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stop = stopwords.words('english')\n",
    "    #write steps here\n",
    "    # lower function\n",
    "    t_1 = lambda x : x.lower()\n",
    "    # Remove punctuation function\n",
    "    t_2 = lambda x : x.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    t_3 = lambda x : \" \".join([w for w in x.split() if w not in stop])\n",
    "    # Snowball stemming\n",
    "    t_4 = lambda x : \" \".join([stemmer.stem(w) for w in x.split()])\n",
    "    # Ngrams with n number of grams\n",
    "    t_5 = lambda x : [\" \".join(ng) for ng in list(ngrams(x.split(), n))]\n",
    "    \n",
    "    \n",
    "    #List of transformation functions\n",
    "    t = [t_1, t_2, t_3, t_4, t_5]\n",
    "    \n",
    "    #Apply transformations\n",
    "    tokenised = reduce(lambda r, f: f(r), t, text)\n",
    "    \n",
    "    return tokenised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d501f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = sum(1 for line in open('True.csv')) - 1\n",
    "n_2 = sum(1 for line in open('Fake.csv')) - 1\n",
    "s = 1500 #desired sample size\n",
    "skip_1 = sorted(random.sample(range(1,n_1+1),n_1-s)) #the 0-indexed header will not be included in the skip list\n",
    "skip_2 = sorted(random.sample(range(1,n_2+1),n_2-s)) #the 0-indexed header will not be included in the skip list\n",
    "\n",
    "raw_data_true = pd.read_csv('True.csv', skiprows=skip_1)\n",
    "raw_data_fake = pd.read_csv('Fake.csv', skiprows=skip_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7dd86a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_true['isFake'] = 0\n",
    "raw_data_fake['isFake'] = 1\n",
    "raw_data = raw_data_true.append(raw_data_fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09af5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t1 = pd.DataFrame()\n",
    "data_t1['article'] = raw_data['title'] + ' ' + raw_data['text']\n",
    "data_t1['isFake'] = raw_data['isFake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1893ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [trump, twitter, dec, 28, global, warm, follow...\n",
       "1       [second, court, reject, trump, bid, stop, tran...\n",
       "2       [hous, panel, ask, trump, extop, aid, bannon, ...\n",
       "3       [green, group, sue, trump, administr, delay, m...\n",
       "4       [republican, appear, certain, pass, tax, legis...\n",
       "                              ...                        \n",
       "1490    [boiler, room, –, ep, 45, –, horror, hotel, tr...\n",
       "1491    [shout, poll, donald, trump, hold, lead, tank,...\n",
       "1492    [zero, knowledg, system, hostil, data, privaci...\n",
       "1493    [arizona, rancher, protest, oregon, target, cp...\n",
       "1494    [mcpain, john, mccain, furious, iran, treat, u...\n",
       "Name: article, Length: 2995, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = data_t1['article'].apply(text_processing, n=1)\n",
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28fb4b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizing...\n",
      "Transforming to tfidf matrix...\n"
     ]
    }
   ],
   "source": [
    "identity = lambda x : x\n",
    "corpus = bag.values\n",
    "print('Count Vectorizing...')\n",
    "vectorizer = CountVectorizer(tokenizer = identity, preprocessor = identity)\n",
    "count_vector = vectorizer.fit_transform(corpus).toarray()\n",
    "print('Transforming to tfidf matrix...')\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "text_tfidf = tfidfTransformer.fit_transform(count_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff024d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(text_tfidf.toarray())\n",
    "y = data_t1['isFake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756f1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408e5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcceb5cf",
   "metadata": {},
   "source": [
    "2.2 Baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b19bfa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeClassifierWithProba(RidgeClassifier):\n",
    "    def predict_proba(self, X):\n",
    "        d = self.decision_function(X)\n",
    "        d_2d = np.c_[-d, d]\n",
    "        \n",
    "        return softmax(d_2d)\n",
    "    \n",
    "models= [LinearDiscriminantAnalysis(), \n",
    "         LogisticRegression(random_state=42), \n",
    "         SGDClassifier(max_iter=1000, tol=1e-3, loss='modified_huber'),\n",
    "         RidgeClassifierWithProba()\n",
    "         #SVC(probability=True)\n",
    "        ]\n",
    "model_names = [\n",
    "    \"Linear Discriminant Analysis\",\n",
    "    \"Logistic Regression\",\n",
    "    \"Stocasthic Gradient Descent\",\n",
    "    \"Ridge\"\n",
    "    #\"SVC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c84992f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model  Linear Discriminant Analysis ...\n",
      "Fitting model  Logistic Regression ...\n",
      "Fitting model  Stocasthic Gradient Descent ...\n",
      "Fitting model  Ridge ...\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i,model in enumerate(models):\n",
    "    print('Fitting model ', model_names[i], '...')\n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    scores.append([\n",
    "        model_names[i],\n",
    "        yhat,\n",
    "        roc_auc_score(y_test, yhat),\n",
    "        f1_score(y_test, yhat),\n",
    "        precision_score(y_test, yhat),\n",
    "        recall_score(y_test, yhat),\n",
    "        accuracy_score(y_test, yhat)\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfa1632e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.896742</td>\n",
       "      <td>0.891608</td>\n",
       "      <td>0.940959</td>\n",
       "      <td>0.847176</td>\n",
       "      <td>0.896494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.961660</td>\n",
       "      <td>0.961345</td>\n",
       "      <td>0.972789</td>\n",
       "      <td>0.950166</td>\n",
       "      <td>0.961603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stocasthic Gradient Descent</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.976627</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.983305</td>\n",
       "      <td>0.983389</td>\n",
       "      <td>0.983389</td>\n",
       "      <td>0.983389</td>\n",
       "      <td>0.983306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  \\\n",
       "0  Linear Discriminant Analysis   \n",
       "1           Logistic Regression   \n",
       "2   Stocasthic Gradient Descent   \n",
       "3                         Ridge   \n",
       "\n",
       "                                         Predictions   ROC AUC  F1-Score  \\\n",
       "0  [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, ...  0.896742  0.891608   \n",
       "1  [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...  0.961660  0.961345   \n",
       "2  [0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...  0.976627  0.976744   \n",
       "3  [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...  0.983305  0.983389   \n",
       "\n",
       "   Precision    Recall  Accuracy  \n",
       "0   0.940959  0.847176  0.896494  \n",
       "1   0.972789  0.950166  0.961603  \n",
       "2   0.976744  0.976744  0.976628  \n",
       "3   0.983389  0.983389  0.983306  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(scores, columns= ['Model', 'Predictions', 'ROC AUC', 'F1-Score', 'Precision', 'Recall', 'Accuracy'])\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d6dff",
   "metadata": {},
   "source": [
    "2.3 Classification Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d981a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the blending ensemble\n",
    "def fit_ensemble(models, X_train, X_val, y_train, y_val, hard=True):\n",
    "    # fit all models on the training set and predict on hold out set\n",
    "    meta_X = list()\n",
    "    for model in models:\n",
    "        # fit in training set\n",
    "        model.fit(X_train, y_train)\n",
    "        # predict on hold out set\n",
    "        yhat = model.predict(X_val) if hard else model.predict_proba(X_val)\n",
    "        # reshape predictions into a matrix with one column\n",
    "        if hard:\n",
    "            yhat = yhat.reshape(len(yhat), 1)\n",
    "        # store predictions as input for blending\n",
    "        meta_X.append(yhat)\n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "    # define blending model\n",
    "    blender = LogisticRegression()\n",
    "    # fit on predictions from base models\n",
    "    blender.fit(meta_X, y_val)\n",
    "    return blender\n",
    "\n",
    "# make a prediction with the blending ensemble\n",
    "def predict_ensemble(models, blender, X_test, hard=True):\n",
    "    # make predictions with base models\n",
    "    meta_X = list()\n",
    "    for model in models:\n",
    "        # predict with base model\n",
    "        yhat = model.predict(X_test) if hard else model.predict_proba(X_test)\n",
    "        # reshape predictions into a matrix with one column\n",
    "        if hard: \n",
    "            yhat = yhat.reshape(len(yhat), 1)\n",
    "        # store prediction\n",
    "        meta_X.append(yhat)\n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "    # predict\n",
    "    return blender.predict(meta_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e4d4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "# split training set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "yhat = predict_ensemble(models, blender, X_test)\n",
    "\n",
    "scores.append([\n",
    "        'Hard Voting Blender',\n",
    "        yhat,\n",
    "        roc_auc_score(y_test, yhat),\n",
    "        f1_score(y_test, yhat),\n",
    "        precision_score(y_test, yhat),\n",
    "        recall_score(y_test, yhat),\n",
    "        accuracy_score(y_test, yhat)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ba73e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val, False)\n",
    "yhat = predict_ensemble(models, blender, X_test, False)\n",
    "\n",
    "scores.append([\n",
    "        'Soft Voting Blender',\n",
    "        yhat,\n",
    "        roc_auc_score(y_test, yhat),\n",
    "        f1_score(y_test, yhat),\n",
    "        precision_score(y_test, yhat),\n",
    "        recall_score(y_test, yhat),\n",
    "        accuracy_score(y_test, yhat)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9057c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each base model\n",
    "def evaluate_models(models, X_train, X_val, y_train, y_val):\n",
    "    # fit and evaluate the models\n",
    "    scores = list()\n",
    "    for model in models:\n",
    "        # fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        # evaluate the model\n",
    "        yhat = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, yhat)\n",
    "        # store the performance\n",
    "        scores.append(acc)\n",
    "    # report model performance\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7570a6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8, 0.9636363636363636, 0.9575757575757575, 0.9797979797979798]\n"
     ]
    }
   ],
   "source": [
    "accuracies = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
    "print(accuracies)\n",
    "ensemble = VotingClassifier(estimators=list(zip(model_names, models)), voting='soft', weights=accuracies)\n",
    "ensemble.fit(X_train, y_train)\n",
    "yhat = ensemble.predict(X_test)\n",
    "\n",
    "scores_bck = scores.copy()\n",
    "scores.append([\n",
    "        'Soft Weighted Ensemble',\n",
    "        yhat,\n",
    "        roc_auc_score(y_test, yhat),\n",
    "        f1_score(y_test, yhat),\n",
    "        precision_score(y_test, yhat),\n",
    "        recall_score(y_test, yhat),\n",
    "        accuracy_score(y_test, yhat)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b47d0f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.896742</td>\n",
       "      <td>0.891608</td>\n",
       "      <td>0.940959</td>\n",
       "      <td>0.847176</td>\n",
       "      <td>0.896494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.961660</td>\n",
       "      <td>0.961345</td>\n",
       "      <td>0.972789</td>\n",
       "      <td>0.950166</td>\n",
       "      <td>0.961603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stocasthic Gradient Descent</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.976627</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>0.983305</td>\n",
       "      <td>0.983389</td>\n",
       "      <td>0.983389</td>\n",
       "      <td>0.983389</td>\n",
       "      <td>0.983306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hard Voting Blender</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.973907</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.968627</td>\n",
       "      <td>0.980159</td>\n",
       "      <td>0.973965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Soft Voting Blender</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.969277</td>\n",
       "      <td>0.969617</td>\n",
       "      <td>0.968338</td>\n",
       "      <td>0.970899</td>\n",
       "      <td>0.969292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Soft Weighted Ensemble</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.971274</td>\n",
       "      <td>0.971617</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.973545</td>\n",
       "      <td>0.971295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  \\\n",
       "0  Linear Discriminant Analysis   \n",
       "1           Logistic Regression   \n",
       "2   Stocasthic Gradient Descent   \n",
       "3                         Ridge   \n",
       "4           Hard Voting Blender   \n",
       "5           Soft Voting Blender   \n",
       "6        Soft Weighted Ensemble   \n",
       "\n",
       "                                         Predictions   ROC AUC  F1-Score  \\\n",
       "0  [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, ...  0.896742  0.891608   \n",
       "1  [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...  0.961660  0.961345   \n",
       "2  [0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...  0.976627  0.976744   \n",
       "3  [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, ...  0.983305  0.983389   \n",
       "4  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.973907  0.974359   \n",
       "5  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.969277  0.969617   \n",
       "6  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.971274  0.971617   \n",
       "\n",
       "   Precision    Recall  Accuracy  \n",
       "0   0.940959  0.847176  0.896494  \n",
       "1   0.972789  0.950166  0.961603  \n",
       "2   0.976744  0.976744  0.976628  \n",
       "3   0.983389  0.983389  0.983306  \n",
       "4   0.968627  0.980159  0.973965  \n",
       "5   0.968338  0.970899  0.969292  \n",
       "6   0.969697  0.973545  0.971295  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(scores, columns= ['Model', 'Predictions', 'ROC AUC', 'F1-Score', 'Precision', 'Recall', 'Accuracy'])\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017fa10b",
   "metadata": {},
   "source": [
    "3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4801055",
   "metadata": {},
   "source": [
    "3.1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4774fea",
   "metadata": {},
   "source": [
    "3.2 Summary and conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
