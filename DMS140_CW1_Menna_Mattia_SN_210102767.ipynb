{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4b7af7",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc7ad86",
   "metadata": {},
   "source": [
    "In the modern era, we are blessed with an enormous quantity of information. Everything can be found online, probably every bit of human knowledge is available with a few clicks, and most of it is completely free.\n",
    "The downside of the connected world is probably the fact that there's no filter, everyone with a smartphone can post information online, and the more this information is repeated and liked (or disliked) in general the more it will be potent and influential (and also profitable), regardless of its veridicality.\n",
    "With the term fake news, we identify any false or misleading information presented as news [[1]](#references), and we have seen them interfere with elections, COVID-19 vaccination programs, and ruin the reputation of many people in the last few years.\n",
    "The problem of automatically detecting fake news it's not an easy one to solve, in this paper we will briefly look at the state of the art and try to add novelty to a particular approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509d1d0",
   "metadata": {},
   "source": [
    "### 1.1 Domain-specific Area\n",
    "\n",
    "According to [[2]](#references) the problem of automatically detecting fake news is not easy to solve because of two factors: \n",
    "\n",
    "1. The Fake news content can be images or video or a podcast, very easy to fake but a lot more complex to analyze and preprocess than normal text.\n",
    "2. There is no way in knowing where people take their information from. The web is full of platforms that provide news and governance is basically non-existent\n",
    "\n",
    "But nonetheless, the ML community has developed a series of solutions to tackle the problem with promising results (at least in the text-based news domain). In the survey by Shivam B. Parikh and Pradeep K. Atrey [[3]](#references) the approaches are divided into six methodology groups: \n",
    "\n",
    "1. Linguistic Features-based Methods, based on the extraction and classification of linguistic features from fake news, usually using a tf-idf representation of the text.\n",
    "2. Deception Modeling based Methods, based on the extraction of the relations between text units on a story as a hierarchical tree.\n",
    "3. Clustering based Methods, based on agglomerative clustering algorithms (such as KNN) trained on a large number of data sets.\n",
    "4. Predictive Modeling based Methods, based on logistic regression and positive or negative coefficients to point out the deception probability of a given text.\n",
    "5. Content Cues based Methods, based on the assumption that the fake news is created solely to engage the readers, unlike real news,  and some form of the linguistic pattern are an indicator of this purpose\n",
    "6. Non-Text Cues based Methods, focuses on the analysis of two non-text components of news: images and user behavior.\n",
    "\n",
    "In this work the focus will be on methodologies of class 1, the text will be processed and stored as a tf-idf matrix and different models will be evaluated against a baseline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002196c",
   "metadata": {},
   "source": [
    "### 1.2 Description of the selected dataset\n",
    "\n",
    "The datasets used for this analysis are three: \n",
    "\n",
    "The <b>LIAR</b> dataset presented in [[4]](#references) and available to the public. It is composed of 12,8 K human labeled short statements from politifact.com labeled with truthfulness ratings: pants-fire, false, barely-true, half-true, mostly-true, and true. The dataset is well balanced and since the analysis will be a binary one (fake news yes/no), to mantain balance we apply the following mapping: \n",
    "\n",
    " - pants-fire: fake news\n",
    " \n",
    " - false: fake news\n",
    "  \n",
    " - barely-true: fake news\n",
    " \n",
    " - half-true: true\n",
    " \n",
    " - mostly-trye: true\n",
    " \n",
    " - true: true\n",
    " \n",
    "\n",
    "The LIAR dataset is downloaded as three tsv files divided into train, test, and validation sets. It is composed of the following columns: \n",
    "\n",
    "1. ID - Text\n",
    "\n",
    "2. Label - Text\n",
    "\n",
    "3. Statement - Text\n",
    "\n",
    "4. Subject - Text\n",
    "\n",
    "5. Speaker - Text\n",
    "\n",
    "6. Speaker Job Title - Text\n",
    "\n",
    "7. State - Text\n",
    "\n",
    "8. Party affiliation - Text - [democrat, republican]\n",
    "\n",
    "9. Barely true count - Integer\n",
    "\n",
    "10. Half true counts - Integer\n",
    "\n",
    "11. Mostly true counts - Integer\n",
    "\n",
    "12. Pants on fire counts - Integer\n",
    "\n",
    "13. Venue/location of the statement - Text\n",
    "\n",
    "\n",
    "The second dataset is the <b>ISOT Fake News Dataset</b>, introduced by Ahmed H, Traore I and Saad S. in [[5]](#references), [[6]](#references) and available on Kaggle [[7]](#references). It is composed of 21417 true news articles and 23481 fake news. The truthful articles were obtained by crawling articles from Reuters.com, and the fake news from different sources, mostly unreliable websites flagged by Politifact and Wikipedia.\n",
    "\n",
    "The ISOT dataset is downloaded as two csv files, true.csv, and fake.csv. It is composed of the following columns: \n",
    "\n",
    "1. Title - Text\n",
    "\n",
    "2. Text - Text\n",
    "\n",
    "3. Subject - Text\n",
    "\n",
    "4. Date - Date\n",
    "\n",
    "\n",
    "Both the described datasets will be reduced to the same format for this analysis: \n",
    "\n",
    "1. Article - Text\n",
    "\n",
    "2. isFake - Boolean\n",
    "\n",
    "\n",
    "From the LIAR dataset we'll sample 3K random rows from the train file and from the ISOT dataset we'll sample 1,5K random rows from the true file and 1,5k rows from the fake file.\n",
    "\n",
    "\n",
    "The third dataset used is a validation dataset and is the concatenation of the previous two datasets. It will have the standard format and it'll be composed of 6K rows.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160dcdd",
   "metadata": {},
   "source": [
    "### 1.3 Objectives\n",
    "\n",
    "The objectives of this project are mainly two: \n",
    "\n",
    "1. Explore different ensemble methodologies with a set of classificators that will be the baseline against which the ensembles will be evaluated. Study the differences between those methodologies and find if there's one best suited to the task of finding fake news. The ensemble techniques that will be used are: \n",
    "\n",
    "    <b>Hard Blending Ensemble</b>, a form of Stacking Generalization without the k-fold cross-validation. We use the predictions of the base models to create a \"meta-model\" that will be then used as training for a \"blending model\" (in our case Logistic Regressor) that will do the actual predictions.\n",
    "\n",
    "    <b>Soft Blending Ensemble</b>, like above, but with the difference that instead of using the predictions of the base models as meta-model we will use the probabilities given by the models as training for the blender\n",
    "\n",
    "    <b>Soft Weighted Voting Ensemble</b>, a form of Voting Ensemble in which the predictions of the base models will result in a prediction based on the majority vote, with a weight given by the accuracy of the single base model on a validation set\n",
    "\n",
    "\n",
    "2. Create an ensemble that can outperform any of the single \"weak learners\" composing the ensemble classificator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda711f",
   "metadata": {},
   "source": [
    "### 1.4 Evaluation Methodology\n",
    "\n",
    "For the evaluation of the algorithms three confusion matrix based scores will be used:\n",
    "\n",
    "<b>Precision</b>\n",
    "\n",
    "Precision is the rate of positive instances that were correctly identified by the classificator over all the positive predicted instances, it is calculated as: \n",
    " $\\frac{TP}{TP + FP}$, where TP is the True Positive and FP is the False Positive. The precision is a measure of the accuracy of the classificator over the positive examples. In the case of a binary classificator a precision of 1 means that all the positive examples predicted by the model are really positive.\n",
    " \n",
    "<b>Recall</b>\n",
    "\n",
    "Recall is the rate of positive instances that were correctly identified by the classificator over all the real positive instances, it is calculated as $\\frac{TP}{TP + FN}$, where FN is the False Negative. In the case of a binary classificator a recall of 1 means that all the really positive examples have been correctly predicted by the model.\n",
    "\n",
    "<b>Accuracy</b>\n",
    "\n",
    "Accuracy is the rate of correct prediction identified by the classificator over the entire sample, it is often expressed as the percentage of correct answers given against a test set. It is calculated as $\\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "<b>F1-Score</b>\n",
    "    \n",
    "F1 Score is the harmonic mean of the precision and recall, it is calculated as  $2*\\frac{Precision*Recall}{Precision+Recall}$. It takes into consideration both precision and recall and gives an overall score of the model. An F-score of 1 means that the model has perfect precision and recall, an f-score of 0 means that either recall or precision is 0.\n",
    "\n",
    "    \n",
    "The evaluation with these metrics will be carried out on a training and test set, both for the baseline and the ensembles. The values of all three metrics are higher for higher performance algorithms and have a range 0-1.\n",
    "The training and test sets will be split as 80% training and 20% test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9318d0",
   "metadata": {},
   "source": [
    "## 2. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d7acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mmenna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Utilities\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import hstack\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "from functools import reduce\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Evaluation Metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48515c2d",
   "metadata": {},
   "source": [
    "### 2.1 Pre-processing\n",
    "\n",
    "The preprocessing of the text will consist of the tokenization of the text and the codification of the entire corpus in a tf-idf matrix.\n",
    "\n",
    "The tokenization of the text is implemented in the custom function \"text-processing\" below, it performs 5 steps:\n",
    "\n",
    "1. Convert text to lower case, to avoid differentiation between words with or without a capital letter\n",
    "2. Remove all punctuation, that is considered not relevant to the analysis\n",
    "3. Remove stopwords, to remove the uninformative pieces of text and reduce the size of the matrix\n",
    "4. Apply stemming with the Snowball Stemmer, to avoid differentiation between different verb forms, plural and singular, etc... for the same word, the word is reduced to his \"stem\" (es. going, go= go)\n",
    "5. Tokenise the text with n grams, to split the text in ngrams\n",
    "\n",
    "Then the entire corpus of tokenized text is converted in a tf-idf matrix with the tfIdfCustomTransformer function below, which leverages on the sklearn libraries.\n",
    "The tf-idf matrix is a numerical matrix that reflects how important a word is to a document in a corpus, every document becomes a vector that is then used as feature to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec98e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text, n = 1):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Convert text to lower case and remove all punctuation\n",
    "    2. Remove stopwords\n",
    "    3. Apply stemming\n",
    "    4. Apply Ngram Tokenisation\n",
    "    5. Returns the tokenised text as a list \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "        String to pre-process\n",
    "    n : int\n",
    "        Number of ngrams for the tokenization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tokenised : list\n",
    "        List containing the tokenised text\n",
    "    \"\"\"\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stop = stopwords.words('english')\n",
    "    #write steps here\n",
    "    # lower function\n",
    "    t_1 = lambda x : x.lower()\n",
    "    # Remove punctuation function\n",
    "    t_2 = lambda x : x.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    t_3 = lambda x : \" \".join([w for w in x.split() if w not in stop])\n",
    "    # Snowball stemming\n",
    "    t_4 = lambda x : \" \".join([stemmer.stem(w) for w in x.split()])\n",
    "    # Ngrams with n number of grams\n",
    "    t_5 = lambda x : [\" \".join(ng) for ng in list(ngrams(x.split(), n))]\n",
    "    \n",
    "    \n",
    "    #List of transformation functions\n",
    "    t = [t_1, t_2, t_3, t_4, t_5]\n",
    "    \n",
    "    #Apply transformations\n",
    "    tokenised = reduce(lambda r, f: f(r), t, text)\n",
    "    \n",
    "    return tokenised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a222ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfIdfCustomTransformer(corpus, preprocessor):\n",
    "    \"\"\"\n",
    "    Takes in an of string, then performs the transformation in a tf-idf matrix with\n",
    "    custom pre-processor and tokenizer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list of string\n",
    "        List of documents to transform to tdf-idf matrix\n",
    "    preprocessor : function\n",
    "        Preprocessing function to apply to the corpus prior\n",
    "        the tf-idf transformation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text_tfidf : matrix\n",
    "        TF-IDF matrix\n",
    "    \"\"\"\n",
    "    identity = lambda x : x\n",
    "    vectorizer = CountVectorizer(tokenizer = identity, preprocessor = identity)\n",
    "    tfidfTransformer = TfidfTransformer()\n",
    "    \n",
    "    bag = [preprocessor(x) for x in corpus]\n",
    "    count_vector = vectorizer.fit_transform(corpus).toarray()\n",
    "    text_tfidf = tfidfTransformer.fit_transform(count_vector)\n",
    "    \n",
    "    return text_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8f221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reding ISOT dataset from CSV\n",
    "isot_raw_data_true = pd.read_csv('True_sample.csv')\n",
    "isot_raw_data_fake = pd.read_csv('Fake_sample.csv')\n",
    "#Creating the isFake column and appending the two files in a single dataframe\n",
    "isot_raw_data_true['isFake'] = 0\n",
    "isot_raw_data_fake['isFake'] = 1\n",
    "isot_raw_data = isot_raw_data_true.append(isot_raw_data_fake)\n",
    "#Reducing the ISOT dataset as a article-isfake format in the i_data_t1 dataframe\n",
    "i_data_t1 = pd.DataFrame()\n",
    "i_data_t1['article'] = isot_raw_data['title'] + ' ' + isot_raw_data['text']\n",
    "i_data_t1['isFake'] = isot_raw_data['isFake']\n",
    "\n",
    "#Reading the LIAR dataset from csv\n",
    "liar_raw_data = pd.read_csv('LIAR_sample.csv')\n",
    "#Mapping the labels to obtain a binary label \n",
    "liar_mapper = {\n",
    "    'false': 1,\n",
    "    'half-true': 0,\n",
    "    'mostly-true': 0,\n",
    "    'true': 0,\n",
    "    'barely-true': 1,\n",
    "    'pants-fire': 1\n",
    "}\n",
    "reduce_fake = lambda x : liar_mapper[x]\n",
    "l_data_t1 = pd.DataFrame()\n",
    "#Reducing the LIAR dataset as a article-isfake format in the l_data_t1 dataframe\n",
    "l_data_t1['article'] = liar_raw_data['Statement']\n",
    "l_data_t1['isFake'] = liar_raw_data['Label'].apply(reduce_fake)\n",
    "\n",
    "#Creating the TOTAL dataset as concatenation of LIAR and ISOT in standard format\n",
    "t_data_t1 = i_data_t1.append(l_data_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b31fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOT dataset features and labels\n",
    "i_X = pd.DataFrame(tfIdfCustomTransformer(i_data_t1['article'].values, text_processing))\n",
    "i_y = i_data_t1['isFake']\n",
    "\n",
    "# LIAR dataset features and labels\n",
    "l_X = pd.DataFrame(tfIdfCustomTransformer(l_data_t1['article'].values, text_processing))\n",
    "l_y = l_data_t1['isFake']\n",
    "\n",
    "# TOTAL dataset features and labels\n",
    "t_X = pd.DataFrame(tfIdfCustomTransformer(t_data_t1['article'].values, text_processing))\n",
    "t_y = t_data_t1['isFake']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcceb5cf",
   "metadata": {},
   "source": [
    "### 2.2 Baseline performance\n",
    "\n",
    "The baseline performance for this work is the performance of 6 of the most commonly used classifiers, some of which, but not all, can be found in the similar work in [[8]](#references): \n",
    "\n",
    "1. **Decision Tree** is a non-parametric supervised learning algorithm used for classification that learns from data to approximate a hierarchical three with a simple decision rule in each node. This tree can be seen as a line in a multidimensional plane that splits the training data into two areas (positive and negative examples).\n",
    "2. **Logistic Regression** is a linear model for classification. It is based on optimizing the parameter of a logistic function to output the probability of a given example to be positive or negative\n",
    "3. **Stochastic Gradient Descent** is an optimizer that uses gradient descent and a loss function to estimate the regression function. In this case, the loss function to be used will be a linear SVM.\n",
    "4. **Ridge Classifier** is a regressor that first converts the binary label in {-1, 1}Â and then treats the problem as a regression task, optimizing a different function than SGD and Logistic Regression.\n",
    "5. **Naive Bayes** is an algorithm that leverages the Bayes' theorem with the assumption of independence between any of the pair of features (a Naive assumption)\n",
    "6. **Support Vector Classificator** is a classifier that finds the best boundary in a hyperplane between the two classes expanding the dimensions of the feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b19bfa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeClassifierWithProba(RidgeClassifier):\n",
    "    def predict_proba(self, X):\n",
    "        d = self.decision_function(X)\n",
    "        d_2d = np.c_[-d, d]\n",
    "        \n",
    "        return softmax(d_2d)\n",
    "    \n",
    "#Baseline models\n",
    "models= [tree.DecisionTreeClassifier(random_state=42), \n",
    "         LogisticRegression(random_state=42), \n",
    "         SGDClassifier(max_iter=1000, tol=1e-3, loss='modified_huber', random_state=42),\n",
    "         RidgeClassifierWithProba(random_state=42),\n",
    "         MultinomialNB(),\n",
    "         SVC(probability=True, random_state=42)\n",
    "        ]\n",
    "model_names = [\n",
    "    \"Decision Tree\",\n",
    "    \"Logistic Regression\",\n",
    "    \"Stocasthic Gradient Descent\",\n",
    "    \"Ridge\",\n",
    "    \"Naive Bayes\",\n",
    "    \"SVC\"\n",
    "]\n",
    "\n",
    "#Columns of the evaluation metric dataframes\n",
    "metric_df_columns = [\n",
    "    'Dataset',\n",
    "    'Model',\n",
    "    'F1-Score',\n",
    "    'Precision',\n",
    "    'Recall',\n",
    "    'Accuracy'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e97e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(ds_name, model_name, y, yhat):\n",
    "    \"\"\"\n",
    "    Takes in a model name, ground truth and predictions \n",
    "    and outputs an array of metrics\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_name : string\n",
    "        Dataset name\n",
    "    model_name : string\n",
    "        String name of the model\n",
    "    y : list\n",
    "        Ground truth\n",
    "    yhat: list\n",
    "        Model predictions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metric : list\n",
    "        List containing the accuracy, f1-score, \n",
    "        precision and recall of the model\n",
    "    \n",
    "    \"\"\"\n",
    "    return [\n",
    "        ds_name,\n",
    "        model_name,\n",
    "        f1_score(y, yhat),\n",
    "        precision_score(y, yhat),\n",
    "        recall_score(y, yhat),\n",
    "        accuracy_score(y, yhat)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d261c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split for the 3 dataset, size is 20-80\n",
    "i_X_train, i_X_test, i_y_train, i_y_test = train_test_split(i_X, i_y, test_size=0.2, random_state=42)\n",
    "l_X_train, l_X_test, l_y_train, l_y_test = train_test_split(l_X, l_y, test_size=0.2, random_state=42)\n",
    "t_X_train, t_X_test, t_y_train, t_y_test = train_test_split(t_X, t_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84992f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model  Decision Tree for Fake News dataset...\n",
      "Fitting model  Decision Tree for Liar dataset...\n",
      "Fitting model  Decision Tree for Total dataset...\n",
      "Fitting model  Logistic Regression for Fake News dataset...\n",
      "Fitting model  Logistic Regression for Liar dataset...\n",
      "Fitting model  Logistic Regression for Total dataset...\n",
      "Fitting model  Stocasthic Gradient Descent for Fake News dataset...\n",
      "Fitting model  Stocasthic Gradient Descent for Liar dataset...\n",
      "Fitting model  Stocasthic Gradient Descent for Total dataset...\n",
      "Fitting model  Ridge for Fake News dataset...\n",
      "Fitting model  Ridge for Liar dataset...\n",
      "Fitting model  Ridge for Total dataset...\n",
      "Fitting model  Naive Bayes for Fake News dataset...\n",
      "Fitting model  Naive Bayes for Liar dataset...\n",
      "Fitting model  Naive Bayes for Total dataset...\n",
      "Fitting model  SVC for Fake News dataset...\n",
      "Fitting model  SVC for Liar dataset...\n",
      "Fitting model  SVC for Total dataset...\n"
     ]
    }
   ],
   "source": [
    "i_scores = []\n",
    "l_scores = []\n",
    "t_scores = []\n",
    "\n",
    "#Evaluating baseline models and storing scores (can take minutes depending on the machine)\n",
    "for i,model in enumerate(models):\n",
    "    # ISOT dataset evaluation\n",
    "    print('Fitting model ', model_names[i], 'for Fake News dataset...')\n",
    "    model.fit(i_X_train, i_y_train)\n",
    "    yhat = model.predict(i_X_test)\n",
    "    i_scores.append(model_metrics('ISOT', model_names[i], i_y_test, yhat))\n",
    "    # LIAR dataset evaluation\n",
    "    print('Fitting model ', model_names[i], 'for Liar dataset...')\n",
    "    model.fit(l_X_train, l_y_train)\n",
    "    yhat = model.predict(l_X_test)\n",
    "    l_scores.append(model_metrics('LIAR', model_names[i], l_y_test, yhat))\n",
    "    #TOTAL dataset evaluation\n",
    "    print('Fitting model ', model_names[i], 'for Total dataset...')\n",
    "    model.fit(t_X_train, t_y_train)\n",
    "    yhat = model.predict(t_X_test)\n",
    "    t_scores.append(model_metrics('TOTAL', model_names[i], t_y_test, yhat))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa1632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_scores_df = pd.DataFrame(i_scores, columns= metric_df_columns)\n",
    "l_scores_df = pd.DataFrame(l_scores, columns= metric_df_columns)\n",
    "t_scores_df = pd.DataFrame(t_scores, columns= metric_df_columns)\n",
    "\n",
    "baseline_i_scores = i_scores.copy()\n",
    "baseline_l_scores = l_scores.copy()\n",
    "baseline_t_scores = t_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d6dff",
   "metadata": {},
   "source": [
    "### 2.3 Classification Approach\n",
    "\n",
    "The classificators that we'll be building are three different flavors of ensembles: \n",
    "\n",
    "1. **Hard Blending Ensemble**, is a type of ensemble algorithm under the stacking generalization. There's is a meta-model that is created using the predictions of the base models, then the \"blender\" is fitted using a classificator model and the final prediction are the prediction of the blender model. In our case, the classificator of choice is the Logistic Regressor. The difference with the stacking generalization is that the meta-model is trained using a holdout dataset instead of k-fold cross-validation. The functions that implement this model are the \"fit_ensemble\" and the \"predict_ensemble\" taken from the beautiful article in [[10]](#references) with some minor modification.\n",
    "2. **Soft Blending Ensemble**, is the same as the hard, the difference is that the meta-model is composed of the probability of the given class instead of the actual prediction. It is implemented with the same functions as above with the difference in the hard parameter\n",
    "3. **Soft Voting Ensemble**, is a simple voting ensemble in which the majority vote of the base models is the model prediction. It is soft because the base model's votes are weighted with the accuracy score of the models themselves on the validation set. The implementation is from sklearn but there's the \"evaluate_models\" function that is used to calculate the accuracies across the base models to be used as weights for the vote\n",
    "\n",
    "The approach is similar to [[9]](#references), with different ensemble methods and base models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d981a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ensemble(models, X_train, X_val, y_train, y_val, hard=True):\n",
    "    \"\"\"\n",
    "    Takes in a list of models, train and validation splits and\n",
    "    performs the fitting of a blending ensemble with Logistic \n",
    "    Regression (hard or soft).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list\n",
    "        List of models\n",
    "    X_train : list\n",
    "        Training features\n",
    "    X_val: list\n",
    "        Validation features\n",
    "    y_train : list\n",
    "        Training labels\n",
    "    y_val : list\n",
    "        Validation labels\n",
    "    hard : boolean\n",
    "        Hard blending or soft blending\n",
    "    Returns\n",
    "    -------\n",
    "    blender : object\n",
    "        The fitted blender model to use for predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    # fit all models on the training set and predict on hold out set\n",
    "    meta_X = list()\n",
    "    for model in models:\n",
    "        # fit in training set\n",
    "        model.fit(X_train, y_train)\n",
    "        # predict on hold out set\n",
    "        yhat = model.predict(X_val) if hard else model.predict_proba(X_val)\n",
    "        # reshape predictions into a matrix with one column\n",
    "        if hard:\n",
    "            yhat = yhat.reshape(len(yhat), 1)\n",
    "        # store predictions as input for blending\n",
    "        meta_X.append(yhat)\n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "    # define blending model\n",
    "    blender = LogisticRegression()\n",
    "    # fit on predictions from base models\n",
    "    blender.fit(meta_X, y_val)\n",
    "    return blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834600e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with the blending ensemble\n",
    "def predict_ensemble(models, blender, X_test, hard=True):\n",
    "    \"\"\"\n",
    "    Takes in a list of models, a fitted blender, test features\n",
    "    and performs the prediction of the labels with the fitted\n",
    "    blender.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list\n",
    "        List of models\n",
    "    blender : object\n",
    "        Fitted blender\n",
    "    X_test : list\n",
    "        Test features\n",
    "    hard : boolean\n",
    "        Hard blending or soft blending\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : list\n",
    "        The predicted labels\n",
    "    \n",
    "    \"\"\"\n",
    "    # make predictions with base models\n",
    "    meta_X = list()\n",
    "    for model in models:\n",
    "        # predict with base model\n",
    "        yhat = model.predict(X_test) if hard else model.predict_proba(X_test)\n",
    "        # reshape predictions into a matrix with one column\n",
    "        if hard: \n",
    "            yhat = yhat.reshape(len(yhat), 1)\n",
    "        # store prediction\n",
    "        meta_X.append(yhat)\n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "    # predict\n",
    "    return blender.predict(meta_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ffbd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each base model\n",
    "def evaluate_models(models, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Takes in a list of models, train and validation splits and\n",
    "    returns the accuracy of the models as an array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list\n",
    "        List of models\n",
    "    X_train : list\n",
    "        Training features\n",
    "    X_val: list\n",
    "        Validation features\n",
    "    y_train : list\n",
    "        Training labels\n",
    "    y_val : list\n",
    "        Validation labels\n",
    "    Returns\n",
    "    -------\n",
    "    accuracies : list\n",
    "        List of accuracies of the models\n",
    "    \n",
    "    \"\"\"\n",
    "    # fit and evaluate the models\n",
    "    scores = list()\n",
    "    for model in models:\n",
    "        # fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        # evaluate the model\n",
    "        yhat = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, yhat)\n",
    "        # store the performance\n",
    "        scores.append(acc)\n",
    "    # report model performance\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94087a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set into train and validation sets ISOT dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(i_X_train, i_y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#Scores backup in case evaluation runs multiple times\n",
    "i_scores = baseline_i_scores.copy()\n",
    "\n",
    "#Fitting and evaluating Hard Blender Ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "yhat = predict_ensemble(models, blender, i_X_test)\n",
    "i_scores.append(model_metrics('ISOT', 'Hard Blender', i_y_test, yhat))\n",
    "\n",
    "#Fitting and evaluationg Soft Blender Ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val, False)\n",
    "yhat = predict_ensemble(models, blender, i_X_test, False)\n",
    "i_scores.append(model_metrics('ISOT', 'Soft Blender', i_y_test, yhat))\n",
    "\n",
    "#Fitting and evaluating Soft Voting Ensemble\n",
    "accuracies = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
    "ensemble = VotingClassifier(estimators=list(zip(model_names, models)), voting='soft', weights=accuracies)\n",
    "ensemble.fit(X_train, y_train)\n",
    "yhat = ensemble.predict(i_X_test)\n",
    "i_scores.append(model_metrics('ISOT', 'Soft Voting Ensemble', i_y_test, yhat))\n",
    "\n",
    "i_scores_df = pd.DataFrame(i_scores, columns= metric_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc2e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set into train and validation sets LIAR dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(l_X_train, l_y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#Scores backup in case evaluation runs multiple times\n",
    "l_scores = baseline_l_scores.copy()\n",
    "\n",
    "#Fitting and evaluating Hard Blender Ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "yhat = predict_ensemble(models, blender, l_X_test)\n",
    "l_scores.append(model_metrics('LIAR', 'Hard Blender', l_y_test, yhat))\n",
    "\n",
    "#Fitting and evaluationg Soft Blender Ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val, False)\n",
    "yhat = predict_ensemble(models, blender, l_X_test, False)\n",
    "l_scores.append(model_metrics('LIAR', 'Soft Blender', l_y_test, yhat))\n",
    "\n",
    "#Fitting and evaluating Soft Voting Ensemble\n",
    "accuracies = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
    "ensemble = VotingClassifier(estimators=list(zip(model_names, models)), voting='soft', weights=accuracies)\n",
    "ensemble.fit(X_train, y_train)\n",
    "yhat = ensemble.predict(l_X_test)\n",
    "l_scores.append(model_metrics('LIAR', 'Soft Voting Ensemble', l_y_test, yhat))\n",
    "\n",
    "l_scores_df = pd.DataFrame(l_scores, columns= metric_df_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e0100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set into train and validation sets TOTAL dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(t_X_train, t_y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#Scores backup in case evaluation runs multiple times\n",
    "t_scores = baseline_t_scores.copy()\n",
    "\n",
    "#Fitting and evaluating Hard Blender Ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "yhat = predict_ensemble(models, blender, t_X_test)\n",
    "t_scores.append(model_metrics('TOTAL', 'Hard Blender', t_y_test, yhat))\n",
    "\n",
    "#Fitting and evaluationg Soft Blender Ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val, False)\n",
    "yhat = predict_ensemble(models, blender, t_X_test, False)\n",
    "t_scores.append(model_metrics('TOTAL', 'Soft Blender', t_y_test, yhat))\n",
    "\n",
    "#Fitting and evaluating Soft Voting Ensemble\n",
    "accuracies = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
    "ensemble = VotingClassifier(estimators=list(zip(model_names, models)), voting='soft', weights=accuracies)\n",
    "ensemble.fit(X_train, y_train)\n",
    "yhat = ensemble.predict(t_X_test)\n",
    "t_scores.append(model_metrics('TOTAL', 'Soft Voting Ensemble', t_y_test, yhat))\n",
    "\n",
    "t_scores_df = pd.DataFrame(t_scores, columns= metric_df_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017fa10b",
   "metadata": {},
   "source": [
    "## 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4801055",
   "metadata": {},
   "source": [
    "### 3.1 Evaluation\n",
    "\n",
    "Below is the report of the metrics evaluated for the six baseline models and the three ensembles.\n",
    "\n",
    "For the **ISOT** dataset we have a clear winner, the probability blending ensemble algorithm performed best in 3 out of 4 metrics with a clear improvement in Accuracy, The Stochastic gradient descent was better in Precision, meaning that is less likely to give a false positive. The scores are very high indicating that the tf-idf representation of the text is a good feature selection method for this kind of long article.\n",
    "\n",
    "For the **LIAR** dataset the scores are very low, slightly above 50% that is the same as a random classificator. The best algorithms were the Decision Tree and the Soft Voting Ensemble with a slightly better score overall for the voting ensemble. The Decision Tree has a better Recall, meaning that is less likely to give a false negative. The statements in the LIAR dataset are very short, probably the tf-idf matrix is not very good at capturing the patterns in the text to label fake news.\n",
    "\n",
    "For the **TOTAL** dataset the best algorithm is the Soft Blender, with the Naive Bayes having a better precision and the SVC having better accuracy. The Soft Blender is more balanced, having a higher f1-score.\n",
    "\n",
    "\n",
    "|    | Dataset   | Model                       |   F1-Score |   Precision |     Recall |   Accuracy |\n",
    "|---:|:----------|:----------------------------|-----------:|------------:|-----------:|-----------:|\n",
    "|  0 | ISOT      | Decision Tree               | 0.916667   |    0.913495 | 0.919861   |   0.92     |\n",
    "|  1 | ISOT      | Logistic Regression         | 0.819149   |    0.833935 | 0.804878   |   0.83     |\n",
    "|  2 | ISOT      | Stocasthic Gradient Descent | 0.894737   |    <b>0.971429</b> | 0.829268   |   0.906667 |\n",
    "|  3 | ISOT      | Ridge                       | 0.868651   |    0.873239 | 0.864111   |   0.875    |\n",
    "|  4 | ISOT      | Naive Bayes                 | 0.640553   |    0.945578 | 0.484321   |   0.74     |\n",
    "|  5 | ISOT      | SVC                         | 0.881834   |    0.892857 | 0.87108    |   0.888333 |\n",
    "|  6 | ISOT      | **Hard Blender**                | 0.928058   |    0.959108 | 0.898955   |   0.933333 |\n",
    "|  7 | ISOT      | **Soft Blender**                | <b>0.955017</b>   |    0.948454 | <b>0.961672</b>   |   <b>0.956667</b> |\n",
    "|  8 | ISOT      | **Soft Voting Ensemble**        | 0.935652   |    0.934028 | 0.937282   |   0.938333 |\n",
    "|----|-----------|-----------------------------|------------|-------------|------------|------------|\n",
    "|  0 | LIAR      | Decision Tree               | **0.46461**    |    0.465455 | **0.463768**   |   0.508333 |\n",
    "|  1 | LIAR      | Logistic Regression         | 0.364066   |    0.52381  | 0.278986   |   0.551667 |\n",
    "|  2 | LIAR      | Stocasthic Gradient Descent | 0.00719424 |    0.5      | 0.00362319 |   0.54     |\n",
    "|  3 | LIAR      | Ridge                       | 0.421053   |    0.533333 | 0.347826   |   0.56     |\n",
    "|  4 | LIAR      | Naive Bayes                 | 0.0070922  |    0.166667 | 0.00362319 |   0.533333 |\n",
    "|  5 | LIAR      | SVC                         | 0.335      |    0.540323 | 0.242754   |   0.556667 |\n",
    "|  6 | LIAR      | **Hard Blender**                | 0.340852   |    0.552846 | 0.246377   |   0.561667 |\n",
    "|  7 | LIAR      | **Soft Blender**                | 0.421053   |    0.571429 | 0.333333   |   0.578333 |\n",
    "|  8 | LIAR      | **Soft Voting Ensemble**        | 0.46087    |    **0.576087** | 0.384058   |   **0.586667** |\n",
    "|----|-----------|-----------------------------|------------|-------------|------------|------------|\n",
    "|  0 | TOTAL     | Decision Tree               | 0.67288    |    0.667283 | 0.678571   |   0.7075   |\n",
    "|  1 | TOTAL     | Logistic Regression         | 0.657084   |    0.723982 | 0.601504   |   0.721667 |\n",
    "|  2 | TOTAL     | Stocasthic Gradient Descent | 0.532676   |    0.774194 | 0.406015   |   0.684167 |\n",
    "|  3 | TOTAL     | Ridge                       | 0.692913   |    0.727273 | 0.661654   |   0.74     |\n",
    "|  4 | TOTAL     | Naive Bayes                 | 0.371345   |    **0.835526** | 0.238722   |   0.641667 |\n",
    "|  5 | TOTAL     | SVC                         | 0.694      |    0.741453 | 0.652256   |   **0.745**    |\n",
    "|  6 | TOTAL     | **Hard Blender**                | 0.687627   |    0.746696 | 0.637218   |   0.743333 |\n",
    "|  7 | TOTAL     | **Soft Blender**                | **0.703669**   |    0.704331 | **0.703008**   |   0.7375   |\n",
    "|  8 | TOTAL     | **Soft Voting Ensemble**        | 0.684418   |    0.719917 | 0.652256   |   0.733333 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a52cbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.913495</td>\n",
       "      <td>0.919861</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.819149</td>\n",
       "      <td>0.833935</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>Stocasthic Gradient Descent</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.868651</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.864111</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.640553</td>\n",
       "      <td>0.945578</td>\n",
       "      <td>0.484321</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.881834</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.871080</td>\n",
       "      <td>0.888333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>Hard Blender</td>\n",
       "      <td>0.928058</td>\n",
       "      <td>0.959108</td>\n",
       "      <td>0.898955</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>Soft Blender</td>\n",
       "      <td>0.955017</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.961672</td>\n",
       "      <td>0.956667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>Soft Voting Ensemble</td>\n",
       "      <td>0.935652</td>\n",
       "      <td>0.934028</td>\n",
       "      <td>0.937282</td>\n",
       "      <td>0.938333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIAR</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.464610</td>\n",
       "      <td>0.465455</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.508333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIAR</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.364066</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.278986</td>\n",
       "      <td>0.551667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIAR</td>\n",
       "      <td>Stocasthic Gradient Descent</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIAR</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIAR</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LIAR</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.540323</td>\n",
       "      <td>0.242754</td>\n",
       "      <td>0.556667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LIAR</td>\n",
       "      <td>Hard Blender</td>\n",
       "      <td>0.340852</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.561667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LIAR</td>\n",
       "      <td>Soft Blender</td>\n",
       "      <td>0.416476</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.329710</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LIAR</td>\n",
       "      <td>Soft Voting Ensemble</td>\n",
       "      <td>0.450766</td>\n",
       "      <td>0.569061</td>\n",
       "      <td>0.373188</td>\n",
       "      <td>0.581667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.672880</td>\n",
       "      <td>0.667283</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.657084</td>\n",
       "      <td>0.723982</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.721667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>Stocasthic Gradient Descent</td>\n",
       "      <td>0.532676</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.684167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.692913</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.661654</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.371345</td>\n",
       "      <td>0.835526</td>\n",
       "      <td>0.238722</td>\n",
       "      <td>0.641667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>0.741453</td>\n",
       "      <td>0.652256</td>\n",
       "      <td>0.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>Hard Blender</td>\n",
       "      <td>0.687627</td>\n",
       "      <td>0.746696</td>\n",
       "      <td>0.637218</td>\n",
       "      <td>0.743333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>Soft Blender</td>\n",
       "      <td>0.703669</td>\n",
       "      <td>0.704331</td>\n",
       "      <td>0.703008</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>Soft Voting Ensemble</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.720497</td>\n",
       "      <td>0.654135</td>\n",
       "      <td>0.734167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset                        Model  F1-Score  Precision    Recall  \\\n",
       "0    ISOT                Decision Tree  0.916667   0.913495  0.919861   \n",
       "1    ISOT          Logistic Regression  0.819149   0.833935  0.804878   \n",
       "2    ISOT  Stocasthic Gradient Descent  0.894737   0.971429  0.829268   \n",
       "3    ISOT                        Ridge  0.868651   0.873239  0.864111   \n",
       "4    ISOT                  Naive Bayes  0.640553   0.945578  0.484321   \n",
       "5    ISOT                          SVC  0.881834   0.892857  0.871080   \n",
       "6    ISOT                 Hard Blender  0.928058   0.959108  0.898955   \n",
       "7    ISOT                 Soft Blender  0.955017   0.948454  0.961672   \n",
       "8    ISOT         Soft Voting Ensemble  0.935652   0.934028  0.937282   \n",
       "0    LIAR                Decision Tree  0.464610   0.465455  0.463768   \n",
       "1    LIAR          Logistic Regression  0.364066   0.523810  0.278986   \n",
       "2    LIAR  Stocasthic Gradient Descent  0.007194   0.500000  0.003623   \n",
       "3    LIAR                        Ridge  0.421053   0.533333  0.347826   \n",
       "4    LIAR                  Naive Bayes  0.007092   0.166667  0.003623   \n",
       "5    LIAR                          SVC  0.335000   0.540323  0.242754   \n",
       "6    LIAR                 Hard Blender  0.340852   0.552846  0.246377   \n",
       "7    LIAR                 Soft Blender  0.416476   0.565217  0.329710   \n",
       "8    LIAR         Soft Voting Ensemble  0.450766   0.569061  0.373188   \n",
       "0   TOTAL                Decision Tree  0.672880   0.667283  0.678571   \n",
       "1   TOTAL          Logistic Regression  0.657084   0.723982  0.601504   \n",
       "2   TOTAL  Stocasthic Gradient Descent  0.532676   0.774194  0.406015   \n",
       "3   TOTAL                        Ridge  0.692913   0.727273  0.661654   \n",
       "4   TOTAL                  Naive Bayes  0.371345   0.835526  0.238722   \n",
       "5   TOTAL                          SVC  0.694000   0.741453  0.652256   \n",
       "6   TOTAL                 Hard Blender  0.687627   0.746696  0.637218   \n",
       "7   TOTAL                 Soft Blender  0.703669   0.704331  0.703008   \n",
       "8   TOTAL         Soft Voting Ensemble  0.685714   0.720497  0.654135   \n",
       "\n",
       "   Accuracy  \n",
       "0  0.920000  \n",
       "1  0.830000  \n",
       "2  0.906667  \n",
       "3  0.875000  \n",
       "4  0.740000  \n",
       "5  0.888333  \n",
       "6  0.933333  \n",
       "7  0.956667  \n",
       "8  0.938333  \n",
       "0  0.508333  \n",
       "1  0.551667  \n",
       "2  0.540000  \n",
       "3  0.560000  \n",
       "4  0.533333  \n",
       "5  0.556667  \n",
       "6  0.561667  \n",
       "7  0.575000  \n",
       "8  0.581667  \n",
       "0  0.707500  \n",
       "1  0.721667  \n",
       "2  0.684167  \n",
       "3  0.740000  \n",
       "4  0.641667  \n",
       "5  0.745000  \n",
       "6  0.743333  \n",
       "7  0.737500  \n",
       "8  0.734167  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = i_scores_df.append(l_scores_df).append(t_scores_df)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4774fea",
   "metadata": {},
   "source": [
    "### 3.2 Summary and conclusions\n",
    "\n",
    "The analysis has presented 6 commonly used algorithms for classification and used them to create an ensemble with three different methodologies. \n",
    "The objective of the paper was to create an ensemble that outperforms the models it is composed of. The objective has been partially met for the LIAR dataset and fully met for the ISOT dataset. The secondary objective was to find out which of the three methodologies to train an ensemble is the best and the answer seems arguable to be the probability blending ensemble. \n",
    "Another reflection must be done on the very low accuracy performance of the algorithms for the LIAR dataset, seems that the tf-idf matrix did not capture enough information from the statements to be anywhere accurate. In this paper, we focused on the text statement ignoring the other interesting columns presented in the dataset. The Non-text attributes of the dataset could be a key factor in training a successful algorithm, the speaker of the statement and the affiliation party can be a key indicators for tweaking the probability of a statement being fake news.\n",
    "\n",
    "This work is further validation of the work presented in [[8]](#references) and [[9]](#references), an additional quantitative proof that the ensembles showed an overall better score as compared to the weak learners.\n",
    "\n",
    "Further extensions on the idea presented in this work could be the hyper-parameter optimization for the learners with cross-fold validation, one of the hyper-parameter being the ensemble components (different sets of classificators). This can be a very slow process if using the entire ISOT dataset for example, and can probably increase the overfitting of the final ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a40b4f",
   "metadata": {},
   "source": [
    "### References\n",
    "<a id=\"references\"></a>\n",
    "[1] https://en.wikipedia.org/wiki/Fake_news\n",
    "\n",
    "[2] Y. Chen, N. J. Conroy, and V. L. Rubin, âNews in an online world: The need for an automatic crap detector,â Proceedings of the Association for Information Science and Technology, vol. 52, no. 1, pp. 1â4, 2015.\n",
    "\n",
    "[3] Parikh, S.B. & Atrey, P.K. 2018, \"Media-Rich Fake News Detection: A Survey\", IEEE, , pp. 436.\n",
    "\n",
    "[4] W. Y. Wang, ââ liar, liar pants on fireâ: A new benchmark dataset for fake news detection,â arXiv preprint\n",
    "arXiv:1705.00648, 2017\n",
    "\n",
    "[5] Ahmed H, Traore I, Saad S. âDetecting opinion spams and fake news using text classificationâ, Journal of Security and Privacy, Volume 1, Issue 1, Wiley, January/February 2018.\n",
    "\n",
    "[6] Ahmed H, Traore I, Saad S. (2017) âDetection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\n",
    "\n",
    "[7] https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "[8] Arvin Hansrajh, Timothy T. Adeliyi, Jeanette Wing, \"Detection of Online Fake News Using Blending Ensemble Learning\", Scientific Programming, vol. 2021, Article ID 3434458, 10 pages, 2021. https://doi.org/10.1155/2021/3434458\n",
    "\n",
    "[9] Iftikhar Ahmad, Muhammad Yousaf, Suhail Yousaf, Muhammad Ovais Ahmad, \"Fake News Detection Using Machine Learning Ensemble Methods\", Complexity, vol. 2020, Article ID 8885861, 11 pages, 2020. https://doi.org/10.1155/2020/8885861\n",
    "\n",
    "[10] https://machinelearningmastery.com/blending-ensemble-machine-learning-with-python/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
